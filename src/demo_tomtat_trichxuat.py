from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
from entity_utils import group_entities, normalize_entities
from summary_utils import summarize_clause
# LOAD MODEL & TOKENIZER
MODEL_DIR = "./models/ner_phobert_full"

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)
model.eval()

id2label = model.config.id2label
# CLEAN TOKEN
def clean_token(token: str) -> str:
    token = token.replace("â–", "")
    token = token.replace("@@", "")
    return token.strip()


# ======================
# PREDICT NER
# ======================
def predict_ner(text: str):
    encoding = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=128
    )

    with torch.no_grad():
        outputs = model(**encoding)

    predictions = torch.argmax(outputs.logits, dim=-1)[0]
    tokens = tokenizer.convert_ids_to_tokens(encoding["input_ids"][0])
    labels = [id2label[p.item()] for p in predictions]

    results = []

    for tok, label in zip(tokens, labels):
        if tok in tokenizer.all_special_tokens:
            continue

        tok = clean_token(tok)
        if tok == "":
            continue

        results.append((tok, label))

    return results

# DEMO
if __name__ == "__main__":
    demo_text = """
    4.2. Náº¿u BÃªn B Ä‘Æ¡n phÆ°Æ¡ng cháº¥m dá»©t há»£p Ä‘á»“ng mÃ  khÃ´ng thá»±c hiá»‡n nghÄ©a vá»¥ bÃ¡o trÆ°á»›c tá»›i BÃªn A thÃ¬ BÃªn A sáº½ khÃ´ng pháº£i hoÃ n tráº£ láº¡i BÃªn B sá»‘ tiá»n Ä‘áº·t cá»c nÃ y.
    Náº¿u BÃªn A Ä‘Æ¡n phÆ°Æ¡ng cháº¥m dá»©t há»£p Ä‘á»“ng mÃ  khÃ´ng thá»±c hiá»‡n nghÄ©a vá»¥ bÃ¡o trÆ°á»›c tá»›i bÃªn B thÃ¬ bÃªn A sáº½ pháº£i hoÃ n tráº£ láº¡i BÃªn B sá»‘ tiá»n Ä‘áº·t cá»c vÃ  pháº£i bá»“i thÆ°á»ng thÃªm má»™t khoáº£n báº±ng chÃ­nh tiá»n Ä‘áº·t cá»c.
    """

    print("ğŸ“„ VÄƒn báº£n Ä‘áº§u vÃ o:")
    print(demo_text.strip())

    # NER
    token_results = predict_ner(demo_text)

    print("\n Káº¿t quáº£ NER (token-level):")
    for token, label in token_results:
        print(f"{token:20s} â†’ {label}")

    # GOM ENTITY
    entities = group_entities(token_results)

    # NORMALIZE ENTITY
    entities = normalize_entities(entities)

    print("\n THá»°C THá»‚ TRÃCH XUáº¤T (ÄÃƒ CHUáº¨N HOÃ):")
    for k, v in entities.items():
        print(f"{k}: {v}")
    summary = summarize_clause(entities)

    print("\n TÃ“M Táº®T ÄIá»€U KHOáº¢N:")
    print(summary)